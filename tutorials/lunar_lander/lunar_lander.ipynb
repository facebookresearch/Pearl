{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cYM7L_EbaD2"
   },
   "source": [
    "# Using DQN and Double DQN in Pearl with different neural network instantiations.\n",
    "\n",
    "Here is a [better rendering](https://nbviewer.org/github/facebookresearch/Pearl/blob/main/tutorials/sequential_decision_making/DQN_and_DoubleDQN_example.ipynb) of this notebook on [nbviewer](https://nbviewer.org/).\n",
    "\n",
    "- The purpose of this tutorial is twofold. First, it illustrates how users can use implementations of value based methods, for example, DQN and Double DQN, in Pearl. We use a simple Gymnasium environment for illustration.\n",
    "\n",
    "- Second, it illustrates how users can instantiate a neural network (outside of a Pearl Agent) and pass it to different policy learners in Pearl. For both examples (DQN and Double DQN), we use an instantiation of `QValueNetworks` outside of the Pearl Agent. The default way right now is to instantiate a Q-value network inside the agent's policy learner.\n",
    "\n",
    "- Users can also instantiate custom networks and use these with different policy learners in Pearl, but are expected to follow the general design of the value networks/critic networks/actor networks base class. For example, for value based methods such as DQN and Double DQN, users should follow the design of the `QValueNetwork` base class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Kztd2SaMY7BK"
   },
   "source": [
    "from sympy.utilities.lambdify import NUMPY\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpBKgJ3tZSKg"
   },
   "source": [
    "# Pearl Installation\n",
    "\n",
    "If you haven't installed Pearl, please make sure you install Pearl with the following cell. Otherwise, you can skip the cell below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFOG6DepZLS1",
    "outputId": "c522edc4-0efc-44b0-e8d0-9ea489758757"
   },
   "source": [
    "# Pearl installation from github. This install also includes PyTorch, Gym and Matplotlib\n",
    "\n",
    "%pip uninstall Pearl -y\n",
    "%rm -rf Pearl\n",
    "!git clone https://github.com/facebookresearch/Pearl.git"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%cd Pearl\n",
    "%pip install .\n",
    "%cd .."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0-qCYjsV71N",
    "outputId": "a1243c8d-5dbe-4ba4-adef-fc09fef72691"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXNni3Xjnbus",
    "outputId": "6f60e961-5a5b-44ba-cd1d-4c789574ec45",
    "ExecuteTime": {
     "end_time": "2024-08-23T20:28:20.451364Z",
     "start_time": "2024-08-23T20:28:19.834109Z"
    }
   },
   "source": "!pip install swig",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (4.2.1)\r\n",
      "Requirement already satisfied: numpy==1.26.4 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (1.26.4)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Gymnasium 0.29.1 does not support Numpy 2+ right now. Either downgrade numpy to version 1+ or wait for Gymnasium 1+\n",
    "# https://github.com/Farama-Foundation/Gymnasium/issues/1142\n",
    "#!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVi9TyChnUqP",
    "outputId": "4e60f038-4eb9-4aac-bad4-7fa829511a03",
    "ExecuteTime": {
     "end_time": "2024-08-23T20:28:21.720624Z",
     "start_time": "2024-08-23T20:28:21.092166Z"
    }
   },
   "source": "!pip install \"gymnasium[box2d]\"",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (0.29.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (3.0.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (0.0.4)\r\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (2.3.5)\r\n",
      "Requirement already satisfied: pygame>=2.1.3 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (2.6.0)\r\n",
      "Requirement already satisfied: swig==4.* in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (4.2.1)\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmM2svESZlWP"
   },
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cTn82t7IZbUz",
    "ExecuteTime": {
     "end_time": "2024-09-05T18:58:18.321728Z",
     "start_time": "2024-09-05T18:58:12.676846Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pearl.neural_networks.sequential_decision_making.q_value_networks import VanillaQValueNetwork\n",
    "from pearl.utils.functional_utils.experimentation.set_seed import set_seed\n",
    "from pearl.policy_learners.sequential_decision_making.double_dqn import DoubleDQN\n",
    "from pearl.policy_learners.sequential_decision_making.ppo import ProximalPolicyOptimization\n",
    "from pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import FIFOOffPolicyReplayBuffer\n",
    "from pearl.policy_learners.exploration_modules.common.epsilon_greedy_exploration import EGreedyExploration\n",
    "from pearl.replay_buffers.sequential_decision_making.on_policy_replay_buffer import OnPolicyReplayBuffer\n",
    "from pearl.utils.functional_utils.train_and_eval.online_learning import online_learning\n",
    "from pearl.pearl_agent import PearlAgent\n",
    "from pearl.policy_learners.sequential_decision_making.ppo_continuous import ContinuousProximalPolicyOptimization\n",
    "from pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n",
    "from pearl.action_representation_modules.one_hot_action_representation_module import (\n",
    "    OneHotActionTensorRepresentationModule,\n",
    ")\n",
    "\n",
    "from importlib.metadata import version\n",
    "\n",
    "set_seed(0)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T18:58:18.344807Z",
     "start_time": "2024-09-05T18:58:18.337467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Python Version: {platform.python_version()}\")\n",
    "print(f\"Torch Version: {version('torch')}\")\n",
    "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Cuda Version: {torch.version.cuda}\")\n",
    "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
    "print(f\"Numpy Version: {version('numpy')}\")\n",
    "print(f\"Pearl Version: {version('pearl')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.10.14\n",
      "Torch Version: 2.4.0\n",
      "Is Cuda Available: False\n",
      "Cuda Version: None\n",
      "Gymnasium Version: 0.29.1\n",
      "Numpy Version: 1.26.4\n",
      "Pearl Version: 0.1.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pw-zKZJHlMAC"
   },
   "source": [
    "# CarPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7iO0RweZmRiQ",
    "outputId": "e8f4cd9e-a711-47eb-e865-8044593d1e34"
   },
   "source": [
    "env_str = \"CartPole-v1\"\n",
    "env = GymEnvironment(env_str)\n",
    "print(f\"Gym Environment: {env_str}\")\n",
    "print(f\"Observation Space: {env.observation_space.shape[0]}\")\n",
    "print(f\"Number of Actions: {env.action_space.n}\")\n",
    "print(f\"Action Space Dimensions: {env.action_space.action_dim}\")\n",
    "print(f\"Is Action Space Continuous: {env.action_space.is_continuous}\")\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRSdX3BwmlCX"
   },
   "source": [
    "## Double DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DQgBO7zKmk84"
   },
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# VanillaQValueNetwork class uses a simple mlp for approximating the Q values.\n",
    "#  - Input dimension of the mlp = (state_dim + action_dim)\n",
    "#  - Size of the intermediate layers are specified as list of `hidden_dims`.\n",
    "hidden_dims = [64, 64]\n",
    "\n",
    "# We will be using a one hot representation for representing actions. So take action_dim = num_actions.\n",
    "Q_value_network = VanillaQValueNetwork(state_dim=env.observation_space.shape[0],  # dimension of the state representation\n",
    "                                       action_dim=env.action_space.n,             # dimension of the action representation\n",
    "                                       hidden_dims=hidden_dims,                   # dimensions of the intermediate layers\n",
    "                                       output_dim=1)                              # set to 1 (Q values are scalars)\n",
    "\n",
    "# Instead of using the 'network_type' argument, use the 'network_instance' argument.\n",
    "# Pass Q_value_network as the `network_instance` to the `DoubleDQN` policy learner.\n",
    "DoubleDQNagent = PearlAgent(\n",
    "    policy_learner=DoubleDQN(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        action_space=env.action_space,\n",
    "        batch_size=64,\n",
    "        training_rounds=10,\n",
    "        soft_update_tau=0.75,\n",
    "        network_instance=Q_value_network,   # pass an instance of Q value network to the policy learner.\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=env.action_space.n\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=FIFOOffPolicyReplayBuffer(10_000),\n",
    ")\n",
    "\n",
    "# The online learning function in Pearl implements environment interaction and learning\n",
    "# and returns a dictionary with episodic returns\n",
    "info_DoubleDQN = online_learning(\n",
    "    agent=DoubleDQNagent,\n",
    "    env=env,\n",
    "    number_of_episodes=200,\n",
    "    print_every_x_episodes=20,   # print returns after every 10 episdoes\n",
    "    learn_after_episode=True,    # instead of updating after every environment interaction, Q networks are updates at the end of each episode\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jyyUwoe6qvPF"
   },
   "source": [
    "# info[\"return\"] refers to the episodic returns\n",
    "torch.save(info_DoubleDQN[\"return\"], \"DoubleDQN-return.pt\")\n",
    "plt.plot(np.arange(len(info_DoubleDQN[\"return\"])),\n",
    "         info_DoubleDQN[\"return\"],\n",
    "         label=\"DoubleDQN\")\n",
    "\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C0Xmte4mCGY"
   },
   "source": [
    "## Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FWyW5kWg1NpL"
   },
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# Create Pearl Agent\n",
    "PPOagent = PearlAgent(\n",
    "    policy_learner=ProximalPolicyOptimization(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        use_critic=True,\n",
    "        action_space=env.action_space,\n",
    "        actor_hidden_dims=[64, 64],\n",
    "        critic_hidden_dims=[64, 64],\n",
    "        batch_size=64,\n",
    "        training_rounds=10,\n",
    "        critic_learning_rate=1e-3,\n",
    "        actor_learning_rate=1e-3,\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=env.action_space.n,\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=OnPolicyReplayBuffer(250_000),\n",
    ")\n",
    "\n",
    "info_PPO = online_learning(\n",
    "    agent=PPOagent,\n",
    "    env=env,\n",
    "    number_of_episodes=750,\n",
    "    print_every_x_episodes=50,\n",
    "    learn_after_episode=True,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SMc5eY11qtG-"
   },
   "source": [
    "# info[\"return\"] refers to the episodic returns\n",
    "torch.save(info_PPO[\"return\"], \"info_PPO-return.pt\")\n",
    "plt.plot(np.arange(len(info_PPO[\"return\"])),\n",
    "         info_PPO[\"return\"],\n",
    "         label=\"PPO\")\n",
    "\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hdz10gMNkRm7"
   },
   "source": [
    "# LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8J7d8Ib-kQ-2",
    "outputId": "3f63f084-5085-41dd-b340-893a25fe10a8",
    "ExecuteTime": {
     "end_time": "2024-09-05T01:14:37.835053Z",
     "start_time": "2024-09-05T01:14:37.827893Z"
    }
   },
   "source": [
    "env_str = \"LunarLander-v2\"\n",
    "env = GymEnvironment(env_str)\n",
    "print(f\"Gym Environment: {env_str}\")\n",
    "print(f\"Observation Space: {env.observation_space.shape[0]}\")\n",
    "print(f\"Number of Actions: {env.action_space.n}\")\n",
    "print(f\"Action Space Dimensions: {env.action_space.action_dim}\")\n",
    "print(f\"Is Action Space Continuous: {env.action_space.is_continuous}\")\n",
    "env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym Environment: LunarLander-v2\n",
      "Observation Space: 8\n",
      "Number of Actions: 4\n",
      "Action Space Dimensions: 1\n",
      "Is Action Space Continuous: False\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLaD5IKGleF8"
   },
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YQ5p1z8WDL58"
   },
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# VanillaQValueNetwork class uses a simple mlp for approximating the Q values.\n",
    "#  - Input dimension of the mlp = (state_dim + action_dim)\n",
    "#  - Size of the intermediate layers are specified as list of `hidden_dims`.\n",
    "hidden_dims = [64, 64]\n",
    "\n",
    "\n",
    "# We will be using a one hot representation for representing actions. So take action_dim = num_actions.\n",
    "Q_value_network = VanillaQValueNetwork(state_dim=env.observation_space.shape[0],  # dimension of the state representation\n",
    "                                       action_dim=env.action_space.n,             # dimension of the action representation\n",
    "                                       hidden_dims=hidden_dims,                   # dimensions of the intermediate layers\n",
    "                                       output_dim=1)                              # set to 1 (Q values are scalars)\n",
    "\n",
    "# Create Pearl Agent\n",
    "# Instead of using the 'network_type' argument, use the 'network_instance' argument.\n",
    "# Pass Q_value_network as the `network_instance` to the `DeepQLearning` policy learner.\n",
    "DoubleDQNagent = PearlAgent(\n",
    "    policy_learner=DoubleDQN(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        action_space=env.action_space,\n",
    "        batch_size=64,\n",
    "        network_instance=Q_value_network,   # pass an instance of Q value network to the policy learner.\n",
    "        exploration_module=EGreedyExploration(0.025),\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=env.action_space.n\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=FIFOOffPolicyReplayBuffer(1_000_000),\n",
    ")\n",
    "\n",
    "info_DoubleDQN = online_learning(\n",
    "    agent=DoubleDQNagent,\n",
    "    env=env,\n",
    "    number_of_episodes=3000,\n",
    "    print_every_x_episodes=50,   # print returns after every 50 episdoes\n",
    "    learn_after_episode=True,    # instead of updating after every environment interaction, Q networks are updates at the end of each episode\n",
    "    record_period=50,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h0s9TOSnqpqM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "outputId": "471dcc1c-efcd-41fc-e8cc-c4a239006133"
   },
   "source": [
    "# info[\"return\"] refers to the episodic returns\n",
    "torch.save(info_DoubleDQN[\"return\"], \"DoubleDQN-return.pt\")\n",
    "plt.plot(np.arange(len(info_DoubleDQN[\"return\"])),\n",
    "         info_DoubleDQN[\"return\"],\n",
    "         label=\"DoubleDQN\")\n",
    "\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqMHuZWklhBb"
   },
   "source": "## Proximal Policy Optimization (PPO) - Discrete Action Spaces"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-LjD7S8Bn7s",
    "outputId": "b471a49a-a531-4be6-9b99-7532cd5c7815"
   },
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# Create Pearl Agent\n",
    "# ProximalPolicyOptimization defaults the exploration module to PropensityExploration\n",
    "PPOagent = PearlAgent(\n",
    "    policy_learner=ProximalPolicyOptimization(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        use_critic=True,\n",
    "        action_space=env.action_space,\n",
    "        actor_hidden_dims=[64, 64],\n",
    "        critic_hidden_dims=[64, 64],\n",
    "        critic_learning_rate=1e-4,\n",
    "        actor_learning_rate=1e-4,\n",
    "        epsilon=0.2,\n",
    "        batch_size=128,\n",
    "        training_rounds=10,\n",
    "        entropy_bonus_scaling=0.01,\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=env.action_space.n,\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=OnPolicyReplayBuffer(250_000),\n",
    ")\n",
    "\n",
    "info_PPO = online_learning(\n",
    "    agent=PPOagent,\n",
    "    env=env,\n",
    "    #number_of_episodes=1000,\n",
    "    number_of_steps=2_000_000,\n",
    "    print_every_x_episodes=100,\n",
    "    #print_every_x_steps=2048,\n",
    "    learn_every_k_steps=2048,\n",
    "    record_period=2048,\n",
    "    learn_after_episode=False,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "khxHZLwFjODv"
   },
   "source": [
    "# info[\"return\"] refers to the episodic returns\n",
    "torch.save(info_PPO[\"return\"], \"info_PPO-return.pt\")\n",
    "plt.plot(np.arange(len(info_PPO[\"return\"])),\n",
    "         info_PPO[\"return\"],\n",
    "         label=\"PPO\")\n",
    "\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LunarLanderContinuous-v2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T18:58:19.251479Z",
     "start_time": "2024-09-05T18:58:18.388635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env_str = \"LunarLanderContinuous-v2\"\n",
    "env = GymEnvironment(env_str)\n",
    "print(f\"Gym Environment: {env_str}\")\n",
    "print(f\"Observation Space: {env.observation_space.shape[0]}\")\n",
    "print(f\"Number of Actions: {env.action_space.shape[0]}\")\n",
    "print(f\"Action Space Dimensions: {env.action_space.action_dim}\")\n",
    "print(f\"Is Action Space Continuous: {env.action_space.is_continuous}\")\n",
    "env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym Environment: LunarLanderContinuous-v2\n",
      "Observation Space: 8\n",
      "Number of Actions: 2\n",
      "Action Space Dimensions: 2\n",
      "Is Action Space Continuous: True\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T19:07:12.010069Z",
     "start_time": "2024-09-05T18:58:19.268067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# Create Pearl Agent\n",
    "PPOagent = PearlAgent(\n",
    "    policy_learner=ContinuousProximalPolicyOptimization(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        use_critic=True,\n",
    "        action_space=env.action_space,\n",
    "        actor_hidden_dims=[64, 64],\n",
    "        critic_hidden_dims=[64, 64],\n",
    "        critic_learning_rate=3e-4,\n",
    "        actor_learning_rate=3e-4,\n",
    "        epsilon=0.2,\n",
    "        batch_size=128,\n",
    "        training_rounds=10,\n",
    "        entropy_bonus_scaling=0.01,\n",
    "    ),\n",
    "    replay_buffer=OnPolicyReplayBuffer(250_000),\n",
    ")\n",
    "\n",
    "info_PPO = online_learning(\n",
    "    agent=PPOagent,\n",
    "    env=env,\n",
    "    #number_of_episodes=1000,\n",
    "    number_of_steps=2_000_000,\n",
    "    print_every_x_episodes=100,\n",
    "    #print_every_x_steps=2048,\n",
    "    learn_every_k_steps=2048,\n",
    "    record_period=2048,\n",
    "    learn_after_episode=False,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100, step 9635, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -374.2360700638965\n",
      "episode 200, step 19176, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -239.36328237317502\n",
      "episode 300, step 29153, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -146.15706675685942\n",
      "episode 400, step 38738, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -414.3755312412977\n",
      "episode 500, step 48623, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -330.50406490522437\n",
      "episode 600, step 58326, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -321.4440070260316\n",
      "episode 700, step 69843, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -139.9222593214363\n",
      "episode 800, step 80108, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -27.058079816401005\n",
      "episode 900, step 90013, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 18.66822591982782\n",
      "episode 1000, step 99791, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -221.63038996234536\n",
      "episode 1100, step 110391, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -142.44459898024797\n",
      "episode 1200, step 120354, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -282.2622927520424\n",
      "episode 1300, step 130084, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -102.21207593940198\n",
      "episode 1400, step 140097, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -73.83264484629035\n",
      "episode 1500, step 149986, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -269.50957494974136\n",
      "episode 1600, step 159609, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -88.06481515616179\n",
      "episode 1700, step 169205, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -140.7689353749156\n",
      "episode 1800, step 179344, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -155.17800844460726\n",
      "episode 1900, step 189119, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -91.35173451740411\n",
      "episode 2000, step 199111, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -298.07731657102704\n",
      "episode 2100, step 211099, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -78.7814284041524\n",
      "episode 2200, step 221712, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -169.68082263413817\n",
      "episode 2300, step 233159, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -198.02271542372182\n",
      "episode 2400, step 244985, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -22.345508951693773\n",
      "episode 2500, step 255371, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -35.80073171108961\n",
      "episode 2600, step 266280, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -1.018966343253851\n",
      "episode 2700, step 277389, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -207.96360589191318\n",
      "episode 2800, step 290910, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -234.21746736206114\n",
      "episode 2900, step 312192, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -17.759600983466953\n",
      "episode 3000, step 329793, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -176.73981033358723\n",
      "episode 3100, step 351031, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -65.30642722919583\n",
      "episode 3200, step 374022, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -323.55221824347973\n",
      "episode 3300, step 394289, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -213.22215673420578\n",
      "episode 3400, step 413595, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -16.40343470312655\n",
      "episode 3500, step 434313, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -136.7828781544231\n",
      "episode 3600, step 453414, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -294.4648361383006\n",
      "episode 3700, step 473710, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -64.70033754833275\n",
      "episode 3800, step 494331, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 11.02846371755004\n",
      "episode 3900, step 516793, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -123.08007757365704\n",
      "episode 4000, step 539673, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -143.75627000350505\n",
      "episode 4100, step 561831, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -222.0845903302543\n",
      "episode 4200, step 589784, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -138.39418723993003\n",
      "episode 4300, step 618567, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -173.50860486435704\n",
      "episode 4400, step 646135, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -222.30776602122933\n",
      "episode 4500, step 675070, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -137.744847425216\n",
      "episode 4600, step 704883, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -4.348060252144933\n",
      "episode 4700, step 733351, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -102.05594587069936\n",
      "episode 4800, step 761720, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 9.13949188683182\n",
      "episode 4900, step 788600, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -320.88993723876774\n",
      "episode 5000, step 819153, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -109.9784474009648\n",
      "episode 5100, step 854744, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -329.6045622192323\n",
      "episode 5200, step 890681, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -475.91347353812307\n",
      "episode 5300, step 932806, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -164.9572686930187\n",
      "episode 5400, step 979926, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 210.10424488122683\n",
      "episode 5500, step 1027598, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 210.0399664250996\n",
      "episode 5600, step 1063764, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 185.6201197293804\n",
      "episode 5700, step 1106777, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -56.887285602802876\n",
      "episode 5800, step 1154485, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 226.2673317240551\n",
      "episode 5900, step 1197248, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 212.7208497264728\n",
      "episode 6000, step 1238807, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -43.31215556501411\n",
      "episode 6100, step 1279850, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 262.5958017345925\n",
      "episode 6200, step 1322642, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 238.13113223684417\n",
      "episode 6300, step 1365820, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 215.95242371666245\n",
      "episode 6400, step 1410542, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 221.0865425439697\n",
      "episode 6500, step 1456536, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 176.83335964229502\n",
      "episode 6600, step 1504486, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 132.97398531265208\n",
      "episode 6700, step 1535408, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 212.52360759515432\n",
      "episode 6800, step 1563192, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 124.18173001951072\n",
      "episode 6900, step 1595951, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -146.41507838945836\n",
      "episode 7000, step 1630133, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -71.47163597447798\n",
      "episode 7100, step 1663389, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -82.41282061347738\n",
      "episode 7200, step 1698433, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -8.531641610432416\n",
      "episode 7300, step 1732976, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -212.90450897387927\n",
      "episode 7400, step 1769540, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 124.0292204557918\n",
      "episode 7500, step 1807163, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 216.05109353198256\n",
      "episode 7600, step 1845335, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: -40.95202488172799\n",
      "episode 7700, step 1883111, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 167.3687072447501\n",
      "episode 7800, step 1921672, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 134.13716188905528\n",
      "episode 7900, step 1960398, agent=PearlAgent with ContinuousProximalPolicyOptimization, OnPolicyReplayBuffer, env=LunarLanderContinuous-v2\n",
      "return: 201.53142641531304\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "fileHeader": "",
  "fileUid": "3c7cd09e-0b1d-4baa-9c96-e7ef3e6154f4",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
