{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cYM7L_EbaD2"
   },
   "source": [
    "# Using DQN and Double DQN in Pearl with different neural network instantiations.\n",
    "\n",
    "Here is a [better rendering](https://nbviewer.org/github/facebookresearch/Pearl/blob/main/tutorials/sequential_decision_making/DQN_and_DoubleDQN_example.ipynb) of this notebook on [nbviewer](https://nbviewer.org/).\n",
    "\n",
    "- The purpose of this tutorial is twofold. First, it illustrates how users can use implementations of value based methods, for example, DQN and Double DQN, in Pearl. We use a simple Gymnasium environment for illustration.\n",
    "\n",
    "- Second, it illustrates how users can instantiate a neural network (outside of a Pearl Agent) and pass it to different policy learners in Pearl. For both examples (DQN and Double DQN), we use an instantiation of `QValueNetworks` outside of the Pearl Agent. The default way right now is to instantiate a Q-value network inside the agent's policy learner.\n",
    "\n",
    "- Users can also instantiate custom networks and use these with different policy learners in Pearl, but are expected to follow the general design of the value networks/critic networks/actor networks base class. For example, for value based methods such as DQN and Double DQN, users should follow the design of the `QValueNetwork` base class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Kztd2SaMY7BK"
   },
   "source": [
    "from sympy.utilities.lambdify import NUMPY\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpBKgJ3tZSKg"
   },
   "source": [
    "# Pearl Installation\n",
    "\n",
    "If you haven't installed Pearl, please make sure you install Pearl with the following cell. Otherwise, you can skip the cell below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFOG6DepZLS1",
    "outputId": "c522edc4-0efc-44b0-e8d0-9ea489758757"
   },
   "source": [
    "# Pearl installation from github. This install also includes PyTorch, Gym and Matplotlib\n",
    "\n",
    "%pip uninstall Pearl -y\n",
    "%rm -rf Pearl\n",
    "!git clone https://github.com/facebookresearch/Pearl.git"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%cd Pearl\n",
    "%pip install .\n",
    "%cd .."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0-qCYjsV71N",
    "outputId": "a1243c8d-5dbe-4ba4-adef-fc09fef72691"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXNni3Xjnbus",
    "outputId": "6f60e961-5a5b-44ba-cd1d-4c789574ec45",
    "ExecuteTime": {
     "end_time": "2024-08-23T20:28:20.451364Z",
     "start_time": "2024-08-23T20:28:19.834109Z"
    }
   },
   "source": "!pip install swig",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (4.2.1)\r\n",
      "Requirement already satisfied: numpy==1.26.4 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (1.26.4)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Gymnasium 0.29.1 does not support Numpy 2+ right now. Either downgrade numpy to version 1+ or wait for Gymnasium 1+\n",
    "# https://github.com/Farama-Foundation/Gymnasium/issues/1142\n",
    "#!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVi9TyChnUqP",
    "outputId": "4e60f038-4eb9-4aac-bad4-7fa829511a03",
    "ExecuteTime": {
     "end_time": "2024-08-23T20:28:21.720624Z",
     "start_time": "2024-08-23T20:28:21.092166Z"
    }
   },
   "source": "!pip install \"gymnasium[box2d]\"",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (0.29.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (3.0.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (0.0.4)\r\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (2.3.5)\r\n",
      "Requirement already satisfied: pygame>=2.1.3 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (2.6.0)\r\n",
      "Requirement already satisfied: swig==4.* in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (4.2.1)\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmM2svESZlWP"
   },
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cTn82t7IZbUz",
    "ExecuteTime": {
     "end_time": "2024-08-28T04:02:59.105448Z",
     "start_time": "2024-08-28T04:02:57.636166Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pearl.neural_networks.sequential_decision_making.q_value_networks import VanillaQValueNetwork\n",
    "from pearl.utils.functional_utils.experimentation.set_seed import set_seed\n",
    "from pearl.policy_learners.sequential_decision_making.double_dqn import DoubleDQN\n",
    "from pearl.policy_learners.sequential_decision_making.ppo import ProximalPolicyOptimization\n",
    "from pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import FIFOOffPolicyReplayBuffer\n",
    "from pearl.policy_learners.exploration_modules.common.epsilon_greedy_exploration import EGreedyExploration\n",
    "from pearl.replay_buffers.sequential_decision_making.on_policy_replay_buffer import OnPolicyReplayBuffer\n",
    "from pearl.utils.functional_utils.train_and_eval.online_learning import online_learning\n",
    "from pearl.pearl_agent import PearlAgent\n",
    "from pearl.policy_learners.sequential_decision_making.ppo_continuous import ContinuousProximalPolicyOptimization\n",
    "from pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n",
    "from pearl.action_representation_modules.one_hot_action_representation_module import (\n",
    "    OneHotActionTensorRepresentationModule,\n",
    ")\n",
    "\n",
    "from importlib.metadata import version\n",
    "\n",
    "set_seed(0)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T04:02:59.124266Z",
     "start_time": "2024-08-28T04:02:59.116656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Python Version: {platform.python_version()}\")\n",
    "print(f\"Torch Version: {version('torch')}\")\n",
    "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Cuda Version: {torch.version.cuda}\")\n",
    "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
    "print(f\"Numpy Version: {version('numpy')}\")\n",
    "print(f\"Pearl Version: {version('pearl')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.10.14\n",
      "Torch Version: 2.4.0\n",
      "Is Cuda Available: False\n",
      "Cuda Version: None\n",
      "Gymnasium Version: 0.29.1\n",
      "Numpy Version: 1.26.4\n",
      "Pearl Version: 0.1.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pw-zKZJHlMAC"
   },
   "source": [
    "# CarPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7iO0RweZmRiQ",
    "outputId": "e8f4cd9e-a711-47eb-e865-8044593d1e34"
   },
   "source": [
    "env_str = \"CartPole-v1\"\n",
    "env = GymEnvironment(env_str)\n",
    "print(f\"Gym Environment: {env_str}\")\n",
    "print(f\"Observation Space: {env.observation_space.shape[0]}\")\n",
    "print(f\"Number of Actions: {env.action_space.n}\")\n",
    "print(f\"Action Space Dimensions: {env.action_space.action_dim}\")\n",
    "print(f\"Is Action Space Continuous: {env.action_space.is_continuous}\")\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRSdX3BwmlCX"
   },
   "source": [
    "## Double DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DQgBO7zKmk84"
   },
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# VanillaQValueNetwork class uses a simple mlp for approximating the Q values.\n",
    "#  - Input dimension of the mlp = (state_dim + action_dim)\n",
    "#  - Size of the intermediate layers are specified as list of `hidden_dims`.\n",
    "hidden_dims = [64, 64]\n",
    "\n",
    "# We will be using a one hot representation for representing actions. So take action_dim = num_actions.\n",
    "Q_value_network = VanillaQValueNetwork(state_dim=env.observation_space.shape[0],  # dimension of the state representation\n",
    "                                       action_dim=env.action_space.n,             # dimension of the action representation\n",
    "                                       hidden_dims=hidden_dims,                   # dimensions of the intermediate layers\n",
    "                                       output_dim=1)                              # set to 1 (Q values are scalars)\n",
    "\n",
    "# Instead of using the 'network_type' argument, use the 'network_instance' argument.\n",
    "# Pass Q_value_network as the `network_instance` to the `DoubleDQN` policy learner.\n",
    "DoubleDQNagent = PearlAgent(\n",
    "    policy_learner=DoubleDQN(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        action_space=env.action_space,\n",
    "        batch_size=64,\n",
    "        training_rounds=10,\n",
    "        soft_update_tau=0.75,\n",
    "        network_instance=Q_value_network,   # pass an instance of Q value network to the policy learner.\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=env.action_space.n\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=FIFOOffPolicyReplayBuffer(10_000),\n",
    ")\n",
    "\n",
    "# The online learning function in Pearl implements environment interaction and learning\n",
    "# and returns a dictionary with episodic returns\n",
    "info_DoubleDQN = online_learning(\n",
    "    agent=DoubleDQNagent,\n",
    "    env=env,\n",
    "    number_of_episodes=200,\n",
    "    print_every_x_episodes=20,   # print returns after every 10 episdoes\n",
    "    learn_after_episode=True,    # instead of updating after every environment interaction, Q networks are updates at the end of each episode\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jyyUwoe6qvPF"
   },
   "source": [
    "# info[\"return\"] refers to the episodic returns\n",
    "torch.save(info_DoubleDQN[\"return\"], \"DoubleDQN-return.pt\")\n",
    "plt.plot(np.arange(len(info_DoubleDQN[\"return\"])),\n",
    "         info_DoubleDQN[\"return\"],\n",
    "         label=\"DoubleDQN\")\n",
    "\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C0Xmte4mCGY"
   },
   "source": [
    "## Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FWyW5kWg1NpL"
   },
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# Create Pearl Agent\n",
    "PPOagent = PearlAgent(\n",
    "    policy_learner=ProximalPolicyOptimization(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        use_critic=True,\n",
    "        action_space=env.action_space,\n",
    "        actor_hidden_dims=[64, 64],\n",
    "        critic_hidden_dims=[64, 64],\n",
    "        batch_size=64,\n",
    "        training_rounds=10,\n",
    "        critic_learning_rate=1e-3,\n",
    "        actor_learning_rate=1e-3,\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=env.action_space.n,\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=OnPolicyReplayBuffer(250_000),\n",
    ")\n",
    "\n",
    "info_PPO = online_learning(\n",
    "    agent=PPOagent,\n",
    "    env=env,\n",
    "    number_of_episodes=750,\n",
    "    print_every_x_episodes=50,\n",
    "    learn_after_episode=True,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SMc5eY11qtG-"
   },
   "source": [
    "# info[\"return\"] refers to the episodic returns\n",
    "torch.save(info_PPO[\"return\"], \"info_PPO-return.pt\")\n",
    "plt.plot(np.arange(len(info_PPO[\"return\"])),\n",
    "         info_PPO[\"return\"],\n",
    "         label=\"PPO\")\n",
    "\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hdz10gMNkRm7"
   },
   "source": [
    "# LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8J7d8Ib-kQ-2",
    "outputId": "3f63f084-5085-41dd-b340-893a25fe10a8",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:36:24.424987Z",
     "start_time": "2024-08-27T21:36:24.302148Z"
    }
   },
   "source": [
    "env_str = \"LunarLander-v2\"\n",
    "env = GymEnvironment(env_str)\n",
    "print(f\"Gym Environment: {env_str}\")\n",
    "print(f\"Observation Space: {env.observation_space.shape[0]}\")\n",
    "print(f\"Number of Actions: {env.action_space.n}\")\n",
    "print(f\"Action Space Dimensions: {env.action_space.action_dim}\")\n",
    "print(f\"Is Action Space Continuous: {env.action_space.is_continuous}\")\n",
    "env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym Environment: LunarLander-v2\n",
      "Observation Space: 8\n",
      "Number of Actions: 4\n",
      "Action Space Dimensions: 1\n",
      "Is Action Space Continuous: False\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLaD5IKGleF8"
   },
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YQ5p1z8WDL58"
   },
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# VanillaQValueNetwork class uses a simple mlp for approximating the Q values.\n",
    "#  - Input dimension of the mlp = (state_dim + action_dim)\n",
    "#  - Size of the intermediate layers are specified as list of `hidden_dims`.\n",
    "hidden_dims = [64, 64]\n",
    "\n",
    "\n",
    "# We will be using a one hot representation for representing actions. So take action_dim = num_actions.\n",
    "Q_value_network = VanillaQValueNetwork(state_dim=env.observation_space.shape[0],  # dimension of the state representation\n",
    "                                       action_dim=env.action_space.n,             # dimension of the action representation\n",
    "                                       hidden_dims=hidden_dims,                   # dimensions of the intermediate layers\n",
    "                                       output_dim=1)                              # set to 1 (Q values are scalars)\n",
    "\n",
    "# Create Pearl Agent\n",
    "# Instead of using the 'network_type' argument, use the 'network_instance' argument.\n",
    "# Pass Q_value_network as the `network_instance` to the `DeepQLearning` policy learner.\n",
    "DoubleDQNagent = PearlAgent(\n",
    "    policy_learner=DoubleDQN(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        action_space=env.action_space,\n",
    "        batch_size=64,\n",
    "        network_instance=Q_value_network,   # pass an instance of Q value network to the policy learner.\n",
    "        exploration_module=EGreedyExploration(0.025),\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=env.action_space.n\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=FIFOOffPolicyReplayBuffer(1_000_000),\n",
    ")\n",
    "\n",
    "info_DoubleDQN = online_learning(\n",
    "    agent=DoubleDQNagent,\n",
    "    env=env,\n",
    "    number_of_episodes=3000,\n",
    "    print_every_x_episodes=50,   # print returns after every 50 episdoes\n",
    "    learn_after_episode=True,    # instead of updating after every environment interaction, Q networks are updates at the end of each episode\n",
    "    record_period=50,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h0s9TOSnqpqM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "outputId": "471dcc1c-efcd-41fc-e8cc-c4a239006133"
   },
   "source": [
    "# info[\"return\"] refers to the episodic returns\n",
    "torch.save(info_DoubleDQN[\"return\"], \"DoubleDQN-return.pt\")\n",
    "plt.plot(np.arange(len(info_DoubleDQN[\"return\"])),\n",
    "         info_DoubleDQN[\"return\"],\n",
    "         label=\"DoubleDQN\")\n",
    "\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqMHuZWklhBb"
   },
   "source": "## Proximal Policy Optimization (PPO) - Discrete Action Spaces"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-LjD7S8Bn7s",
    "outputId": "b471a49a-a531-4be6-9b99-7532cd5c7815"
   },
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# Create Pearl Agent\n",
    "# ProximalPolicyOptimization defaults the exploration module to PropensityExploration\n",
    "PPOagent = PearlAgent(\n",
    "    policy_learner=ProximalPolicyOptimization(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        use_critic=True,\n",
    "        action_space=env.action_space,\n",
    "        actor_hidden_dims=[64, 64],\n",
    "        critic_hidden_dims=[64, 64],\n",
    "        critic_learning_rate=3e-4,\n",
    "        actor_learning_rate=3e-4,\n",
    "        epsilon=0.2,\n",
    "        batch_size=128,\n",
    "        training_rounds=100,\n",
    "        entropy_bonus_scaling=0.01,\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=env.action_space.n,\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=OnPolicyReplayBuffer(250_000),\n",
    ")\n",
    "\n",
    "info_PPO = online_learning(\n",
    "    agent=PPOagent,\n",
    "    env=env,\n",
    "    #number_of_episodes=1000,\n",
    "    number_of_steps=2_000_000,\n",
    "    print_every_x_episodes=100,\n",
    "    #print_every_x_steps=2048,\n",
    "    learn_every_k_steps=2048,\n",
    "    record_period=2048,\n",
    "    learn_after_episode=False,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "khxHZLwFjODv"
   },
   "source": [
    "# info[\"return\"] refers to the episodic returns\n",
    "torch.save(info_PPO[\"return\"], \"info_PPO-return.pt\")\n",
    "plt.plot(np.arange(len(info_PPO[\"return\"])),\n",
    "         info_PPO[\"return\"],\n",
    "         label=\"PPO\")\n",
    "\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LunarLanderContinuous-v2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T21:43:19.920156Z",
     "start_time": "2024-08-27T21:43:19.797787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env_str = \"LunarLanderContinuous-v2\"\n",
    "env = GymEnvironment(env_str)\n",
    "print(f\"Gym Environment: {env_str}\")\n",
    "print(f\"Observation Space: {env.observation_space.shape[0]}\")\n",
    "print(f\"Number of Actions: {env.action_space.shape[0]}\")\n",
    "print(f\"Action Space Dimensions: {env.action_space.action_dim}\")\n",
    "print(f\"Is Action Space Continuous: {env.action_space.is_continuous}\")\n",
    "env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym Environment: LunarLanderContinuous-v2\n",
      "Observation Space: 8\n",
      "Number of Actions: 2\n",
      "Action Space Dimensions: 2\n",
      "Is Action Space Continuous: True\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# Create Pearl Agent\n",
    "PPOagent = PearlAgent(\n",
    "    policy_learner=ContinuousProximalPolicyOptimization(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        use_critic=True,\n",
    "        action_space=env.action_space,\n",
    "        actor_hidden_dims=[64, 64],\n",
    "        critic_hidden_dims=[64, 64],\n",
    "        critic_learning_rate=1e-4,\n",
    "        actor_learning_rate=1e-4,\n",
    "        epsilon=0.2,\n",
    "        batch_size=128,\n",
    "        training_rounds=100,\n",
    "        entropy_bonus_scaling=0.01,\n",
    "    ),\n",
    "    replay_buffer=OnPolicyReplayBuffer(250_000),\n",
    ")\n",
    "\n",
    "info_PPO = online_learning(\n",
    "    agent=PPOagent,\n",
    "    env=env,\n",
    "    #number_of_episodes=1000,\n",
    "    number_of_steps=2_000_000,\n",
    "    print_every_x_episodes=100,\n",
    "    #print_every_x_steps=2048,\n",
    "    learn_every_k_steps=2048,\n",
    "    record_period=2048,\n",
    "    learn_after_episode=False,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "fileHeader": "",
  "fileUid": "3c7cd09e-0b1d-4baa-9c96-e7ef3e6154f4",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
