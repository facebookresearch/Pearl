{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cYM7L_EbaD2"
   },
   "source": [
    "# Using DQN and Double DQN in Pearl with different neural network instantiations.\n",
    "\n",
    "Here is a [better rendering](https://nbviewer.org/github/facebookresearch/Pearl/blob/main/tutorials/sequential_decision_making/DQN_and_DoubleDQN_example.ipynb) of this notebook on [nbviewer](https://nbviewer.org/).\n",
    "\n",
    "- The purpose of this tutorial is twofold. First, it illustrates how users can use implementations of value based methods, for example, DQN and Double DQN, in Pearl. We use a simple Gym environment for illustration.\n",
    "\n",
    "- Second, it illustrates how users can instantiate a neural network (outside of a Pearl Agent) and pass it to different policy learners in Pearl. For both examples (DQN and Double DQN), we use an instantiation of `QValueNetworks` outside of the Pearl Agent. The default way right now is to instantiate a Q-value network inside the agent's policy learner.\n",
    "\n",
    "- Users can also instantiate custom networks and use these with different policy learners in Pearl, but are expected to follow the general design of the value networks/critic networks/actor networks base class. For example, for value based methods such as DQN and Double DQN, users should follow the design of the `QValueNetwork` base class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Kztd2SaMY7BK"
   },
   "source": [
    "from sympy.utilities.lambdify import NUMPY\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpBKgJ3tZSKg"
   },
   "source": [
    "# Pearl Installation\n",
    "\n",
    "If you haven't installed Pearl, please make sure you install Pearl with the following cell. Otherwise, you can skip the cell below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFOG6DepZLS1",
    "outputId": "c522edc4-0efc-44b0-e8d0-9ea489758757"
   },
   "source": [
    "# Pearl installation from github. This install also includes PyTorch, Gym and Matplotlib\n",
    "\n",
    "%pip uninstall Pearl -y\n",
    "%rm -rf Pearl\n",
    "!git clone https://github.com/facebookresearch/Pearl.git"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%cd Pearl\n",
    "%pip install .\n",
    "%cd .."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0-qCYjsV71N",
    "outputId": "a1243c8d-5dbe-4ba4-adef-fc09fef72691"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXNni3Xjnbus",
    "outputId": "6f60e961-5a5b-44ba-cd1d-4c789574ec45",
    "ExecuteTime": {
     "end_time": "2024-08-23T20:28:20.451364Z",
     "start_time": "2024-08-23T20:28:19.834109Z"
    }
   },
   "source": "!pip install swig",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (4.2.1)\r\n",
      "Requirement already satisfied: numpy==1.26.4 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (1.26.4)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Gymnasium 0.29.1 does not support Numpy 2+ right. Either downgrade numpy to version 1+ or wait for Gymnasium 1+\n",
    "#!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVi9TyChnUqP",
    "outputId": "4e60f038-4eb9-4aac-bad4-7fa829511a03",
    "ExecuteTime": {
     "end_time": "2024-08-23T20:28:21.720624Z",
     "start_time": "2024-08-23T20:28:21.092166Z"
    }
   },
   "source": "!pip install \"gymnasium[box2d]\"",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (0.29.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (3.0.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (0.0.4)\r\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (2.3.5)\r\n",
      "Requirement already satisfied: pygame>=2.1.3 in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (2.6.0)\r\n",
      "Requirement already satisfied: swig==4.* in /Users/michaelkudlaty/anaconda3/envs/PPOPearl/lib/python3.10/site-packages (from gymnasium[box2d]) (4.2.1)\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmM2svESZlWP"
   },
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cTn82t7IZbUz",
    "ExecuteTime": {
     "end_time": "2024-08-23T20:32:39.295827Z",
     "start_time": "2024-08-23T20:32:37.742664Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pearl.neural_networks.sequential_decision_making.q_value_networks import VanillaQValueNetwork\n",
    "from pearl.utils.functional_utils.experimentation.set_seed import set_seed\n",
    "from pearl.policy_learners.sequential_decision_making.double_dqn import DoubleDQN\n",
    "from pearl.policy_learners.sequential_decision_making.ppo import ProximalPolicyOptimization\n",
    "from pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import FIFOOffPolicyReplayBuffer\n",
    "from pearl.policy_learners.exploration_modules.common.epsilon_greedy_exploration import EGreedyExploration\n",
    "from pearl.replay_buffers.sequential_decision_making.on_policy_replay_buffer import OnPolicyReplayBuffer\n",
    "from pearl.utils.functional_utils.train_and_eval.online_learning import online_learning\n",
    "from pearl.pearl_agent import PearlAgent\n",
    "from pearl.policy_learners.sequential_decision_making.ppo_continuous import ContinuousProximalPolicyOptimization\n",
    "from pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n",
    "from pearl.action_representation_modules.one_hot_action_representation_module import (\n",
    "    OneHotActionTensorRepresentationModule,\n",
    ")\n",
    "\n",
    "set_seed(0)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T20:32:56.379620Z",
     "start_time": "2024-08-23T20:32:56.377061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Gymnasium Version: {gymnasium.__version__}\")\n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "print(f\"Numpy Version: {np.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gymnasium Version: 0.29.1\n",
      "Torch Version: 2.4.0\n",
      "Numpy Version: 1.26.4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pw-zKZJHlMAC"
   },
   "source": [
    "# CarPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7iO0RweZmRiQ",
    "outputId": "e8f4cd9e-a711-47eb-e865-8044593d1e34"
   },
   "source": [
    "env_str = \"CartPole-v1\"\n",
    "env = GymEnvironment(env_str)\n",
    "print(f\"Gym Environment: {env_str}\")\n",
    "print(f\"Observation Space: {env.observation_space.shape[0]}\")\n",
    "print(f\"Number of Actions: {env.action_space.n}\")\n",
    "print(f\"Action Space Dimensions: {env.action_space.action_dim}\")\n",
    "print(f\"Is Action Space Continuous: {env.action_space.is_continuous}\")\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRSdX3BwmlCX"
   },
   "source": [
    "## Double DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DQgBO7zKmk84"
   },
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# VanillaQValueNetwork class uses a simple mlp for approximating the Q values.\n",
    "#  - Input dimension of the mlp = (state_dim + action_dim)\n",
    "#  - Size of the intermediate layers are specified as list of `hidden_dims`.\n",
    "hidden_dims = [64, 64]\n",
    "\n",
    "# We will be using a one hot representation for representing actions. So take action_dim = num_actions.\n",
    "Q_value_network = VanillaQValueNetwork(state_dim=env.observation_space.shape[0],  # dimension of the state representation\n",
    "                                       action_dim=env.action_space.n,             # dimension of the action representation\n",
    "                                       hidden_dims=hidden_dims,                   # dimensions of the intermediate layers\n",
    "                                       output_dim=1)                              # set to 1 (Q values are scalars)\n",
    "\n",
    "# Instead of using the 'network_type' argument, use the 'network_instance' argument.\n",
    "# Pass Q_value_network as the `network_instance` to the `DoubleDQN` policy learner.\n",
    "DoubleDQNagent = PearlAgent(\n",
    "    policy_learner=DoubleDQN(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        action_space=env.action_space,\n",
    "        batch_size=64,\n",
    "        training_rounds=10,\n",
    "        soft_update_tau=0.75,\n",
    "        network_instance=Q_value_network,   # pass an instance of Q value network to the policy learner.\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=env.action_space.n\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=FIFOOffPolicyReplayBuffer(10_000),\n",
    ")\n",
    "\n",
    "# The online learning function in Pearl implements environment interaction and learning\n",
    "# and returns a dictionary with episodic returns\n",
    "info_DoubleDQN = online_learning(\n",
    "    agent=DoubleDQNagent,\n",
    "    env=env,\n",
    "    number_of_episodes=200,\n",
    "    print_every_x_episodes=20,   # print returns after every 10 episdoes\n",
    "    learn_after_episode=True,    # instead of updating after every environment interaction, Q networks are updates at the end of each episode\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jyyUwoe6qvPF"
   },
   "source": [
    "# info[\"return\"] refers to the episodic returns\n",
    "torch.save(info_DoubleDQN[\"return\"], \"DoubleDQN-return.pt\")\n",
    "plt.plot(np.arange(len(info_DoubleDQN[\"return\"])),\n",
    "         info_DoubleDQN[\"return\"],\n",
    "         label=\"DoubleDQN\")\n",
    "\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C0Xmte4mCGY"
   },
   "source": [
    "## Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FWyW5kWg1NpL"
   },
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# Create Pearl Agent\n",
    "PPOagent = PearlAgent(\n",
    "    policy_learner=ProximalPolicyOptimization(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        use_critic=True,\n",
    "        action_space=env.action_space,\n",
    "        actor_hidden_dims=[64, 64],\n",
    "        critic_hidden_dims=[64, 64],\n",
    "        batch_size=64,\n",
    "        training_rounds=10,\n",
    "        critic_learning_rate=1e-3,\n",
    "        actor_learning_rate=1e-3,\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=env.action_space.n,\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=OnPolicyReplayBuffer(250_000),\n",
    ")\n",
    "\n",
    "info_PPO = online_learning(\n",
    "    agent=PPOagent,\n",
    "    env=env,\n",
    "    number_of_episodes=750,\n",
    "    print_every_x_episodes=50,\n",
    "    learn_after_episode=True,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SMc5eY11qtG-"
   },
   "source": [
    "# info[\"return\"] refers to the episodic returns\n",
    "torch.save(info_PPO[\"return\"], \"info_PPO-return.pt\")\n",
    "plt.plot(np.arange(len(info_PPO[\"return\"])),\n",
    "         info_PPO[\"return\"],\n",
    "         label=\"PPO\")\n",
    "\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hdz10gMNkRm7"
   },
   "source": [
    "# LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8J7d8Ib-kQ-2",
    "outputId": "3f63f084-5085-41dd-b340-893a25fe10a8"
   },
   "source": [
    "env_str = \"LunarLander-v2\"\n",
    "env = GymEnvironment(env_str)\n",
    "print(f\"Gym Environment: {env_str}\")\n",
    "print(f\"Observation Space: {env.observation_space.shape[0]}\")\n",
    "print(f\"Number of Actions: {env.action_space.n}\")\n",
    "print(f\"Action Space Dimensions: {env.action_space.action_dim}\")\n",
    "print(f\"Is Action Space Continuous: {env.action_space.is_continuous}\")\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLaD5IKGleF8"
   },
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YQ5p1z8WDL58"
   },
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# VanillaQValueNetwork class uses a simple mlp for approximating the Q values.\n",
    "#  - Input dimension of the mlp = (state_dim + action_dim)\n",
    "#  - Size of the intermediate layers are specified as list of `hidden_dims`.\n",
    "hidden_dims = [64, 64]\n",
    "\n",
    "\n",
    "# We will be using a one hot representation for representing actions. So take action_dim = num_actions.\n",
    "Q_value_network = VanillaQValueNetwork(state_dim=env.observation_space.shape[0],  # dimension of the state representation\n",
    "                                       action_dim=env.action_space.n,             # dimension of the action representation\n",
    "                                       hidden_dims=hidden_dims,                   # dimensions of the intermediate layers\n",
    "                                       output_dim=1)                              # set to 1 (Q values are scalars)\n",
    "\n",
    "# Create Pearl Agent\n",
    "# Instead of using the 'network_type' argument, use the 'network_instance' argument.\n",
    "# Pass Q_value_network as the `network_instance` to the `DeepQLearning` policy learner.\n",
    "DoubleDQNagent = PearlAgent(\n",
    "    policy_learner=DoubleDQN(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        action_space=env.action_space,\n",
    "        batch_size=64,\n",
    "        network_instance=Q_value_network,   # pass an instance of Q value network to the policy learner.\n",
    "        exploration_module=EGreedyExploration(0.025),\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=env.action_space.n\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=FIFOOffPolicyReplayBuffer(1_000_000),\n",
    ")\n",
    "\n",
    "info_DoubleDQN = online_learning(\n",
    "    agent=DoubleDQNagent,\n",
    "    env=env,\n",
    "    number_of_episodes=3000,\n",
    "    print_every_x_episodes=50,   # print returns after every 50 episdoes\n",
    "    learn_after_episode=True,    # instead of updating after every environment interaction, Q networks are updates at the end of each episode\n",
    "    record_period=50,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h0s9TOSnqpqM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "outputId": "471dcc1c-efcd-41fc-e8cc-c4a239006133"
   },
   "source": [
    "# info[\"return\"] refers to the episodic returns\n",
    "torch.save(info_DoubleDQN[\"return\"], \"DoubleDQN-return.pt\")\n",
    "plt.plot(np.arange(len(info_DoubleDQN[\"return\"])),\n",
    "         info_DoubleDQN[\"return\"],\n",
    "         label=\"DoubleDQN\")\n",
    "\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqMHuZWklhBb"
   },
   "source": "## Proximal Policy Optimization (PPO) - Discrete Action Spaces"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-LjD7S8Bn7s",
    "outputId": "b471a49a-a531-4be6-9b99-7532cd5c7815"
   },
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# Create Pearl Agent\n",
    "PPOagent = PearlAgent(\n",
    "    policy_learner=ProximalPolicyOptimization(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        use_critic=True,\n",
    "        action_space=env.action_space,\n",
    "        actor_hidden_dims=[64, 64],\n",
    "        critic_hidden_dims=[64, 64],\n",
    "        critic_learning_rate=3e-4,\n",
    "        actor_learning_rate=3e-4,\n",
    "        epsilon=0.2,\n",
    "        batch_size=128,\n",
    "        training_rounds=100,\n",
    "        entropy_bonus_scaling=0.01,\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=env.action_space.n,\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=OnPolicyReplayBuffer(250_000),\n",
    ")\n",
    "\n",
    "info_PPO = online_learning(\n",
    "    agent=PPOagent,\n",
    "    env=env,\n",
    "    #number_of_episodes=1000,\n",
    "    number_of_steps=2_000_000,\n",
    "    print_every_x_episodes=100,\n",
    "    #print_every_x_steps=2048,\n",
    "    learn_every_k_steps=2048,\n",
    "    record_period=2048,\n",
    "    learn_after_episode=False,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "khxHZLwFjODv"
   },
   "source": [
    "# info[\"return\"] refers to the episodic returns\n",
    "torch.save(info_PPO[\"return\"], \"info_PPO-return.pt\")\n",
    "plt.plot(np.arange(len(info_PPO[\"return\"])),\n",
    "         info_PPO[\"return\"],\n",
    "         label=\"PPO\")\n",
    "\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LunarLanderContinuous-v2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T20:33:04.184753Z",
     "start_time": "2024-08-23T20:33:04.103239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env_str = \"LunarLanderContinuous-v2\"\n",
    "env = GymEnvironment(env_str)\n",
    "print(f\"Gym Environment: {env_str}\")\n",
    "print(f\"Observation Space: {env.observation_space.shape[0]}\")\n",
    "print(f\"Number of Actions: {env.action_space.shape[0]}\")\n",
    "print(f\"Action Space Dimensions: {env.action_space.action_dim}\")\n",
    "print(f\"Is Action Space Continuous: {env.action_space.is_continuous}\")\n",
    "env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym Environment: LunarLanderContinuous-v2\n",
      "Observation Space: 8\n",
      "Number of Actions: 2\n",
      "Action Space Dimensions: 2\n",
      "Is Action Space Continuous: True\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T20:39:26.815160Z",
     "start_time": "2024-08-23T20:33:07.696033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Environment\n",
    "env = GymEnvironment(env_str)\n",
    "\n",
    "# Create Pearl Agent\n",
    "PPOagent = PearlAgent(\n",
    "    policy_learner=ContinuousProximalPolicyOptimization(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        use_critic=True,\n",
    "        action_space=env.action_space,\n",
    "        actor_hidden_dims=[64, 64],\n",
    "        critic_hidden_dims=[64, 64],\n",
    "        critic_learning_rate=3e-4,\n",
    "        actor_learning_rate=3e-4,\n",
    "        epsilon=0.2,\n",
    "        batch_size=128,\n",
    "        training_rounds=100,\n",
    "        entropy_bonus_scaling=0.01,\n",
    "    ),\n",
    "    replay_buffer=OnPolicyReplayBuffer(250_000),\n",
    ")\n",
    "\n",
    "info_PPO = online_learning(\n",
    "    agent=PPOagent,\n",
    "    env=env,\n",
    "    #number_of_episodes=1000,\n",
    "    number_of_steps=2_000_000,\n",
    "    print_every_x_episodes=100,\n",
    "    #print_every_x_steps=2048,\n",
    "    learn_every_k_steps=2048,\n",
    "    record_period=2048,\n",
    "    learn_after_episode=False,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (128, 2)) of distribution Normal(loc: torch.Size([128, 2]), scale: torch.Size([128, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 22\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Create Pearl Agent\u001B[39;00m\n\u001B[1;32m      5\u001B[0m PPOagent \u001B[38;5;241m=\u001B[39m PearlAgent(\n\u001B[1;32m      6\u001B[0m     policy_learner\u001B[38;5;241m=\u001B[39mContinuousProximalPolicyOptimization(\n\u001B[1;32m      7\u001B[0m         state_dim\u001B[38;5;241m=\u001B[39menv\u001B[38;5;241m.\u001B[39mobservation_space\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     19\u001B[0m     replay_buffer\u001B[38;5;241m=\u001B[39mOnPolicyReplayBuffer(\u001B[38;5;241m250_000\u001B[39m),\n\u001B[1;32m     20\u001B[0m )\n\u001B[0;32m---> 22\u001B[0m info_PPO \u001B[38;5;241m=\u001B[39m \u001B[43monline_learning\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mPPOagent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#number_of_episodes=1000,\u001B[39;49;00m\n\u001B[1;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnumber_of_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2_000_000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprint_every_x_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#print_every_x_steps=2048,\u001B[39;49;00m\n\u001B[1;32m     29\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearn_every_k_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2048\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrecord_period\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2048\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearn_after_episode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\n\u001B[1;32m     33\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m env\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/PycharmProjects/Pearl/pearl/utils/functional_utils/train_and_eval/online_learning.py:113\u001B[0m, in \u001B[0;36monline_learning\u001B[0;34m(agent, env, number_of_episodes, number_of_steps, learn_after_episode, learn_every_k_steps, print_every_x_episodes, print_every_x_steps, seed, record_period)\u001B[0m\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    112\u001B[0m old_total_steps \u001B[38;5;241m=\u001B[39m total_steps\n\u001B[0;32m--> 113\u001B[0m episode_info, episode_total_steps \u001B[38;5;241m=\u001B[39m \u001B[43mrun_episode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[43m    \u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[43m    \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexploit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearn_after_episode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearn_after_episode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearn_every_k_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearn_every_k_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtotal_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mold_total_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m number_of_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m episode_total_steps \u001B[38;5;241m>\u001B[39m record_period:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    125\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn episode is longer than the record_period: episode length \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepisode_total_steps\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    126\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, record_period \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrecord_period\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Try using a larger record_period.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    127\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/Pearl/pearl/utils/functional_utils/train_and_eval/online_learning.py:296\u001B[0m, in \u001B[0;36mrun_episode\u001B[0;34m(agent, env, learn, exploit, learn_after_episode, learn_every_k_steps, total_steps, seed)\u001B[0m\n\u001B[1;32m    294\u001B[0m             \u001B[38;5;28;01massert\u001B[39;00m learn_every_k_steps \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlearn_every_k_steps must be positive\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    295\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m (total_steps \u001B[38;5;241m+\u001B[39m episode_steps) \u001B[38;5;241m%\u001B[39m learn_every_k_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 296\u001B[0m                 \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    298\u001B[0m info \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn\u001B[39m\u001B[38;5;124m\"\u001B[39m: cum_reward}\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_risky_sa \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;66;03m# pyre-fixme[6]: For 1st argument expected `SupportsKeysAndGetItem[str,\u001B[39;00m\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;66;03m#  int]` but got `Dict[str, float]`.\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Pearl/pearl/pearl_agent.py:209\u001B[0m, in \u001B[0;36mPearlAgent.learn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    208\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[0;32m--> 209\u001B[0m     report \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_learner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplay_buffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msafety_module\u001B[38;5;241m.\u001B[39mlearn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplay_buffer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_learner)\n\u001B[1;32m    212\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_learner\u001B[38;5;241m.\u001B[39mon_policy:\n",
      "File \u001B[0;32m~/PycharmProjects/Pearl/pearl/policy_learners/sequential_decision_making/ppo_continuous.py:150\u001B[0m, in \u001B[0;36mContinuousProximalPolicyOptimization.learn\u001B[0;34m(self, replay_buffer)\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess_replay_buffer(replay_buffer)\n\u001B[1;32m    149\u001B[0m \u001B[38;5;66;03m# Sample from replay buffer and learn\u001B[39;00m\n\u001B[0;32m--> 150\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreplay_buffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;66;03m# Update old actor with the latest actor for the next round\u001B[39;00m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/PycharmProjects/Pearl/pearl/policy_learners/policy_learner.py:178\u001B[0m, in \u001B[0;36mPolicyLearner.learn\u001B[0;34m(self, replay_buffer)\u001B[0m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(batch, TransitionBatch):\n\u001B[1;32m    177\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess_batch(batch)\n\u001B[0;32m--> 178\u001B[0m     single_report \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m single_report\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m report:\n",
      "File \u001B[0;32m~/PycharmProjects/Pearl/pearl/policy_learners/sequential_decision_making/actor_critic_base.py:288\u001B[0m, in \u001B[0;36mActorCriticBase.learn_batch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn_batch\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch: TransitionBatch) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[1;32m    267\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;124;03m    Trains the actor and critic networks using a batch of transitions.\u001B[39;00m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;124;03m    This method performs the following steps:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;124;03m        and actor updates. These can be useful to track for debugging purposes.\u001B[39;00m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 288\u001B[0m     actor_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_actor_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_actor_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    290\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;124;03m    If the history summarization module is a neural network,\u001B[39;00m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;124;03m    the computation graph of this neural network is used\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    295\u001B[0m \u001B[38;5;124;03m    After the graph is cleared, critic_loss.backward() fails.\u001B[39;00m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Pearl/pearl/policy_learners/sequential_decision_making/ppo_continuous.py:118\u001B[0m, in \u001B[0;36mContinuousProximalPolicyOptimization._actor_loss\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    115\u001B[0m action_std \u001B[38;5;241m=\u001B[39m action_log_std\u001B[38;5;241m.\u001B[39mexp()\n\u001B[1;32m    117\u001B[0m \u001B[38;5;66;03m# Create a normal distribution based on the mean and std\u001B[39;00m\n\u001B[0;32m--> 118\u001B[0m dist \u001B[38;5;241m=\u001B[39m \u001B[43mNormal\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction_mean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_std\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m log_probs \u001B[38;5;241m=\u001B[39m dist\u001B[38;5;241m.\u001B[39mlog_prob(batch\u001B[38;5;241m.\u001B[39maction)\u001B[38;5;241m.\u001B[39msum(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    121\u001B[0m \u001B[38;5;66;03m# Calculate the ratio for PPO\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/PPOPearl/lib/python3.10/site-packages/torch/distributions/normal.py:57\u001B[0m, in \u001B[0;36mNormal.__init__\u001B[0;34m(self, loc, scale, validate_args)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     56\u001B[0m     batch_shape \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloc\u001B[38;5;241m.\u001B[39msize()\n\u001B[0;32m---> 57\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/PPOPearl/lib/python3.10/site-packages/torch/distributions/distribution.py:70\u001B[0m, in \u001B[0;36mDistribution.__init__\u001B[0;34m(self, batch_shape, event_shape, validate_args)\u001B[0m\n\u001B[1;32m     68\u001B[0m         valid \u001B[38;5;241m=\u001B[39m constraint\u001B[38;5;241m.\u001B[39mcheck(value)\n\u001B[1;32m     69\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid\u001B[38;5;241m.\u001B[39mall():\n\u001B[0;32m---> 70\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     71\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected parameter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     72\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(value)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtuple\u001B[39m(value\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     73\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof distribution \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     74\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto satisfy the constraint \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(constraint)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     75\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut found invalid values:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     76\u001B[0m             )\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[0;31mValueError\u001B[0m: Expected parameter loc (Tensor of shape (128, 2)) of distribution Normal(loc: torch.Size([128, 2]), scale: torch.Size([128, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "fileHeader": "",
  "fileUid": "3c7cd09e-0b1d-4baa-9c96-e7ef3e6154f4",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
